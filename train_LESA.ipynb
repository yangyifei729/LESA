{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from copy import deepcopy\n",
    "from tqdm import trange\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your model, we use Meta-Llama-3-8B for example\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "\n",
    "# load Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='balanced_low_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $\\mathcal{W_G}$ and prepare for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def prepare_data_sliding_window(VT_sep):\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "\n",
    "    for i in range(1, len(VT_sep) - 1):\n",
    "        V1 = VT_sep[i - 1]\n",
    "        V2 = VT_sep[i]\n",
    "        V3 = VT_sep[i + 1]\n",
    "\n",
    "        X = torch.cat([V1, V3], dim=1)\n",
    "        Y = V2\n",
    "\n",
    "        X_list.append(X)\n",
    "        Y_list.append(Y)\n",
    "\n",
    "    X = torch.cat(X_list, dim=0)\n",
    "    Y = torch.cat(Y_list, dim=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensor_list_as_file(tensor_list, file_path):\n",
    "    torch.save(tensor_list, file_path)\n",
    "    print(f\"Saved tensor list to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the matrices names\n",
    "weight_matrices = [\n",
    "    'mlp.down_proj.weight.data',\n",
    "    'mlp.up_proj.weight.data',\n",
    "    'mlp.gate_proj.weight.data',\n",
    "    'self_attn.q_proj.weight.data',\n",
    "    'self_attn.k_proj.weight.data',\n",
    "    'self_attn.v_proj.weight.data',\n",
    "    'self_attn.o_proj.weight.data'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train $\\mathcal{W_G}$ and use it to predict the layers to inserted into model between 15-31 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_select_num = len(model.model.layers)\n",
    "\n",
    "top_k = 1\n",
    "\n",
    "for weight_name in weight_matrices:\n",
    "    print(f'weight_name:{weight_name}')\n",
    "    concatenated_list = []\n",
    "    for layer_idx, layer in enumerate(tqdm(model.model.layers[0:layer_select_num])):\n",
    "        weight = eval(f\"layer.{weight_name}\")\n",
    "        concatenated_list.append(weight)\n",
    "\n",
    "    concatenated_list = [t.to('cuda:0') for t in concatenated_list]\n",
    "    concatenated_matrix = torch.cat(concatenated_list, dim=1)\n",
    "    concatenated_matrix = concatenated_matrix.to('cuda:0')\n",
    "\n",
    "    U, S, VT = torch.linalg.svd(concatenated_matrix, full_matrices=False)\n",
    "    U, S, VT = U.cpu(), S.cpu(), VT.cpu()\n",
    "    Sigma = torch.diag(S)\n",
    "    if Sigma.size(0) < U.size(1):\n",
    "        Sigma = torch.cat([Sigma, torch.zeros(U.size(1) - Sigma.size(0), S.size(0))], dim=0)\n",
    "    if Sigma.size(1) < VT.size(0):\n",
    "        Sigma = torch.cat([Sigma, torch.zeros(Sigma.size(0), VT.size(0) - Sigma.size(1))], dim=1)\n",
    "\n",
    "    y_shape = concatenated_list[0].shape[1]\n",
    "\n",
    "    VT_sep = [VT[:, i * y_shape : (i + 1) * y_shape] for i in range(layer_select_num)]\n",
    "\n",
    "    X, Y = prepare_data_sliding_window(VT_sep)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    hidden_dim = 256\n",
    "    output_dim = Y.shape[1]\n",
    "    pred_model = SimpleNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.AdamW(pred_model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pred_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_X, batch_Y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = pred_model(batch_X)\n",
    "            mse_loss = criterion(outputs, batch_Y)\n",
    "            norm_loss = criterion(torch.norm(outputs, p=2), torch.norm(batch_Y, p=2))\n",
    "            lambda_reg = 0.0005\n",
    "            loss = (1 - lambda_reg) * mse_loss + lambda_reg * norm_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(dataloader):.6f}\")\n",
    "\n",
    "\n",
    "    pred_v_ls = []\n",
    "    pred_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(15, 31):\n",
    "            V1 = VT_sep[idx].to(device)\n",
    "            V3 = VT_sep[idx+1].to(device)\n",
    "\n",
    "            X_eval = torch.cat([V1, V3], dim=1)\n",
    "\n",
    "            predictions = pred_model(X_eval)\n",
    "            pred_v_ls.append(predictions)\n",
    "    SVD_mean_recons_ls = [(U * S) @ pred_v.to('cpu') for pred_v in pred_v_ls]\n",
    "    print(torch.norm(SVD_mean_recons_ls[0]))\n",
    "    weight_layer0 = eval(f'model.model.layers[0].{weight_name}')\n",
    "    print(torch.norm(weight_layer0))\n",
    "\n",
    "    save_dir = 'YOUR_SAVE_PATH'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    save_path = f'{save_dir}{weight_name.split(\".\")[1]}_15_31_inter.pt'\n",
    "    save_tensor_list_as_file(SVD_mean_recons_ls, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
